{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69512b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/packages/minerva-centos7/py_packages/3.7/lib/python3.7/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "usage: ipykernel_launcher.py [-h] [--run_prefix RUN_PREFIX] [--train] [--test]\n",
      "                             [--dev] [--threshold THRESHOLD]\n",
      "                             [--ensemble_paths [ENSEMBLE_PATHS [ENSEMBLE_PATHS ...]]]\n",
      "                             [--train_years [TRAIN_YEARS [TRAIN_YEARS ...]]]\n",
      "                             [--dev_years [DEV_YEARS [DEV_YEARS ...]]]\n",
      "                             [--test_years [TEST_YEARS [TEST_YEARS ...]]]\n",
      "                             [--predict_birads]\n",
      "                             [--predict_birads_lambda PREDICT_BIRADS_LAMBDA]\n",
      "                             [--invasive_only] [--rebalance_eval_cancers]\n",
      "                             [--downsample_activ]\n",
      "                             [--confidence_interval CONFIDENCE_INTERVAL]\n",
      "                             [--num_resamples NUM_RESAMPLES]\n",
      "                             [--dataset DATASET]\n",
      "                             [--image_transformers [IMAGE_TRANSFORMERS [IMAGE_TRANSFORMERS ...]]]\n",
      "                             [--tensor_transformers [TENSOR_TRANSFORMERS [TENSOR_TRANSFORMERS ...]]]\n",
      "                             [--test_image_transformers [TEST_IMAGE_TRANSFORMERS [TEST_IMAGE_TRANSFORMERS ...]]]\n",
      "                             [--test_tensor_transformers [TEST_TENSOR_TRANSFORMERS [TEST_TENSOR_TRANSFORMERS ...]]]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--img_size IMG_SIZE [IMG_SIZE ...]]\n",
      "                             [--patch_size PATCH_SIZE [PATCH_SIZE ...]]\n",
      "                             [--get_dataset_stats]\n",
      "                             [--get_activs_instead_of_hiddens]\n",
      "                             [--img_mean IMG_MEAN [IMG_MEAN ...]]\n",
      "                             [--img_std IMG_STD [IMG_STD ...]]\n",
      "                             [--img_dir IMG_DIR] [--num_chan NUM_CHAN]\n",
      "                             [--force_input_dim] [--input_dim INPUT_DIM]\n",
      "                             [--transfomer_hidden_dim TRANSFOMER_HIDDEN_DIM]\n",
      "                             [--num_heads NUM_HEADS] [--multi_image]\n",
      "                             [--num_images NUM_IMAGES] [--pred_both_sides]\n",
      "                             [--min_num_images MIN_NUM_IMAGES] [--video]\n",
      "                             [--metadata_dir METADATA_DIR]\n",
      "                             [--metadata_path METADATA_PATH]\n",
      "                             [--cache_path CACHE_PATH] [--drop_benign_side]\n",
      "                             [--class_bal]\n",
      "                             [--shift_class_bal_towards_imediate_cancers]\n",
      "                             [--year_weighted_class_bal] [--device_class_bal]\n",
      "                             [--allowed_devices [ALLOWED_DEVICES [ALLOWED_DEVICES ...]]]\n",
      "                             [--use_c_view_if_available]\n",
      "                             [--use_spatial_transformer]\n",
      "                             [--spatial_transformer_name SPATIAL_TRANSFORMER_NAME]\n",
      "                             [--spatial_transformer_img_size SPATIAL_TRANSFORMER_IMG_SIZE [SPATIAL_TRANSFORMER_IMG_SIZE ...]]\n",
      "                             [--location_network_name LOCATION_NETWORK_NAME]\n",
      "                             [--location_network_block_layout LOCATION_NETWORK_BLOCK_LAYOUT [LOCATION_NETWORK_BLOCK_LAYOUT ...]]\n",
      "                             [--tps_grid_size TPS_GRID_SIZE]\n",
      "                             [--tps_span_range TPS_SPAN_RANGE]\n",
      "                             [--use_region_annotation]\n",
      "                             [--fraction_region_annotation_to_use FRACTION_REGION_ANNOTATION_TO_USE]\n",
      "                             [--region_annotation_loss_type REGION_ANNOTATION_LOSS_TYPE]\n",
      "                             [--region_annotation_pred_kernel_size REGION_ANNOTATION_PRED_KERNEL_SIZE]\n",
      "                             [--region_annotation_focal_loss_lambda REGION_ANNOTATION_FOCAL_LOSS_LAMBDA]\n",
      "                             [--region_annotation_contrast_alpha REGION_ANNOTATION_CONTRAST_ALPHA]\n",
      "                             [--regularization_lambda REGULARIZATION_LAMBDA]\n",
      "                             [--use_adv] [--use_mmd_adv] [--add_repulsive_mmd]\n",
      "                             [--use_temporal_mmd]\n",
      "                             [--temporal_mmd_cache_size TEMPORAL_MMD_CACHE_SIZE]\n",
      "                             [--temporal_mmd_discount_factor TEMPORAL_MMD_DISCOUNT_FACTOR]\n",
      "                             [--adv_loss_lambda ADV_LOSS_LAMBDA]\n",
      "                             [--train_adv_seperate] [--anneal_adv_loss]\n",
      "                             [--turn_off_model_train] [--adv_on_logits_alone]\n",
      "                             [--num_model_steps NUM_MODEL_STEPS]\n",
      "                             [--num_adv_steps NUM_ADV_STEPS] [--wrap_model]\n",
      "                             [--use_risk_factors] [--pred_risk_factors]\n",
      "                             [--pred_risk_factors_lambda PRED_RISK_FACTORS_LAMBDA]\n",
      "                             [--use_pred_risk_factors_at_test]\n",
      "                             [--use_pred_risk_factors_if_unk]\n",
      "                             [--risk_factor_keys [RISK_FACTOR_KEYS [RISK_FACTOR_KEYS ...]]]\n",
      "                             [--risk_factor_metadata_path RISK_FACTOR_METADATA_PATH]\n",
      "                             [--survival_analysis_setup] [--make_probs_indep]\n",
      "                             [--mask_mechanism MASK_MECHANISM]\n",
      "                             [--eval_survival_on_risk]\n",
      "                             [--max_followup MAX_FOLLOWUP]\n",
      "                             [--eval_risk_survival] [--mask_prob MASK_PROB]\n",
      "                             [--pred_missing_mammos]\n",
      "                             [--also_pred_given_mammos]\n",
      "                             [--pred_missing_mammos_lambda PRED_MISSING_MAMMOS_LAMBDA]\n",
      "                             [--use_precomputed_hiddens] [--zero_out_hiddens]\n",
      "                             [--use_precomputed_hiddens_in_get_hiddens]\n",
      "                             [--hiddens_results_path HIDDENS_RESULTS_PATH]\n",
      "                             [--use_dev_to_train_model_on_hiddens]\n",
      "                             [--turn_off_init_projection]\n",
      "                             [--optimizer OPTIMIZER] [--objective OBJECTIVE]\n",
      "                             [--init_lr INIT_LR] [--momentum MOMENTUM]\n",
      "                             [--lr_decay LR_DECAY]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--patience PATIENCE] [--turn_off_model_reset]\n",
      "                             [--tuning_metric TUNING_METRIC] [--epochs EPOCHS]\n",
      "                             [--max_batches_per_train_epoch MAX_BATCHES_PER_TRAIN_EPOCH]\n",
      "                             [--max_batches_per_dev_epoch MAX_BATCHES_PER_DEV_EPOCH]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--batch_splits BATCH_SPLITS] [--dropout DROPOUT]\n",
      "                             [--save_dir SAVE_DIR]\n",
      "                             [--results_path RESULTS_PATH]\n",
      "                             [--prediction_save_path PREDICTION_SAVE_PATH]\n",
      "                             [--no_tuning_on_dev]\n",
      "                             [--lr_reduction_interval LR_REDUCTION_INTERVAL]\n",
      "                             [--data_fraction DATA_FRACTION]\n",
      "                             [--ten_fold_cross_val]\n",
      "                             [--ten_fold_cross_val_seed TEN_FOLD_CROSS_VAL_SEED]\n",
      "                             [--ten_fold_test_index TEN_FOLD_TEST_INDEX]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--num_layers NUM_LAYERS] [--snapshot SNAPSHOT]\n",
      "                             [--state_dict_path STATE_DICT_PATH]\n",
      "                             [--img_encoder_snapshot IMG_ENCODER_SNAPSHOT]\n",
      "                             [--freeze_image_encoder]\n",
      "                             [--transformer_snapshot TRANSFORMER_SNAPSHOT]\n",
      "                             [--callibrator_snapshot CALLIBRATOR_SNAPSHOT]\n",
      "                             [--patch_snapshot PATCH_SNAPSHOT]\n",
      "                             [--pretrained_on_imagenet]\n",
      "                             [--pretrained_imagenet_model_name PRETRAINED_IMAGENET_MODEL_NAME]\n",
      "                             [--make_fc] [--replace_bn_with_gn]\n",
      "                             [--block_layout BLOCK_LAYOUT [BLOCK_LAYOUT ...]]\n",
      "                             [--block_widening_factor BLOCK_WIDENING_FACTOR]\n",
      "                             [--num_groups NUM_GROUPS] [--pool_name POOL_NAME]\n",
      "                             [--deep_risk_factor_pool]\n",
      "                             [--replace_snapshot_pool] [--is_ccds_server]\n",
      "                             [--cuda] [--num_gpus NUM_GPUS]\n",
      "                             [--num_shards NUM_SHARDS] [--data_parallel]\n",
      "                             [--model_parallel] [--plot_losses]\n",
      "                             [--cluster_exams]\n",
      "                             [--background_size BACKGROUND_SIZE [BACKGROUND_SIZE ...]]\n",
      "                             [--noise] [--noise_var NOISE_VAR]\n",
      "                             [--use_permissive_cohort]\n",
      "                             [--mammogram_type MAMMOGRAM_TYPE] [--resume]\n",
      "                             [--ignore_warnings]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /hpc/users/sistia01/.local/share/jupyter/runtime/kernel-59f7f684-39d9-44b5-a544-13b096e956d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "from os.path import dirname, realpath\n",
    "import sys\n",
    "#import git\n",
    "#sys.path.append(dirname(dirname(realpath(__file__))))\n",
    "import torch\n",
    "import onconet.datasets.factory as dataset_factory\n",
    "import onconet.models.factory as model_factory\n",
    "from onconet.learn import train\n",
    "import onconet.transformers.factory as transformer_factory\n",
    "import onconet.visualize as visualize\n",
    "import onconet.utils.parsing as parsing\n",
    "import warnings\n",
    "import onconet.learn.state_keeper as state\n",
    "from onconet.utils.get_dataset_stats import get_dataset_stats\n",
    "import onconet.utils.stats as stats\n",
    "import pdb\n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "#import joblib\n",
    "sys.modules[\"sklearn.svm.classes\"] = sklearn.svm #added\n",
    "sys.modules[\"sklearn.preprocessing.label\"] = sklearn.preprocessing #added\n",
    "\n",
    "#Constants\n",
    "DATE_FORMAT_STR = \"%Y-%m-%d:%H-%M-%S\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parsing.parse_args()\n",
    "    if args.ignore_warnings:\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    commit  = repo.head.object\n",
    "    args.commit = commit.hexsha\n",
    "    print(\"OncoNet main running from commit: \\n\\n{}\\n{}author: {}, date: {}\".format(\n",
    "        commit.hexsha, commit.message, commit.author, commit.committed_date))\n",
    "\n",
    "    if args.get_dataset_stats:\n",
    "        print(\"\\nComputing image mean and std...\")\n",
    "        args.img_mean, args.img_std = get_dataset_stats(args)\n",
    "        print('Mean: {}'.format(args.img_mean))\n",
    "        print('Std: {}'.format(args.img_std))\n",
    "\n",
    "    print(\"\\nLoading data-augmentation scheme...\")\n",
    "    transformers = transformer_factory.get_transformers(\n",
    "        args.image_transformers, args.tensor_transformers, args)\n",
    "    test_transformers = transformer_factory.get_transformers(\n",
    "        args.test_image_transformers, args.test_tensor_transformers, args)\n",
    "    # Load dataset and add dataset specific information to args\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_data, dev_data, test_data = dataset_factory.get_dataset(args, transformers, test_transformers)\n",
    "    # Load model and add model specific information to args\n",
    "    if args.snapshot is None:\n",
    "        model = model_factory.get_model(args)\n",
    "    else:\n",
    "        model = model_factory.load_model(args.snapshot, args)\n",
    "        if args.replace_snapshot_pool:\n",
    "            non_trained_model = model_factory.get_model(args)\n",
    "            model._model.pool = non_trained_model._model.pool\n",
    "            model._model.args = non_trained_model._model.args\n",
    "\n",
    "\n",
    "    print(model)\n",
    "    # Load run parameters if resuming that run.\n",
    "    args.model_path = state.get_model_path(args)\n",
    "    print('Trained model will be saved to [%s]' % args.model_path)\n",
    "    if args.resume:\n",
    "        try:\n",
    "            state_keeper = state.StateKeeper(args)\n",
    "            model, optimizer_state, epoch, lr, epoch_stats = state_keeper.load()\n",
    "            args.optimizer_state = optimizer_state\n",
    "            args.current_epoch = epoch\n",
    "            args.lr = lr\n",
    "            args.epoch_stats = epoch_stats\n",
    "        except:\n",
    "            args.optimizer_state = None\n",
    "            args.current_epoch = None\n",
    "            args.lr = None\n",
    "            args.epoch_stats = None\n",
    "            print(\"\\n Error loading previous state. \\n Starting run from scratch.\")\n",
    "    else:\n",
    "        print(\"\\n Restarting run from scratch.\")\n",
    "\n",
    "\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(args.__dict__.items()):\n",
    "        if attr not in ['optimizer_state', 'patient_to_partition_dict', 'path_to_hidden_dict', 'exam_to_year_dict', 'exam_to_device_dict']:\n",
    "            print(\"\\t{}={}\".format(attr.upper(), value))\n",
    "\n",
    "    save_path = args.results_path\n",
    "    print()\n",
    "    if args.train:\n",
    "        epoch_stats, model = train.train_model(train_data, dev_data, model, args)\n",
    "        args.epoch_stats = epoch_stats\n",
    "\n",
    "        if args.plot_losses:\n",
    "            visualize.viz_utils.plot_losses(epoch_stats)\n",
    "        print(\"Save train/dev results to {}\".format(save_path))\n",
    "        args_dict = vars(args)\n",
    "        pickle.dump(args_dict, open(save_path, 'wb'))\n",
    "\n",
    "    print()\n",
    "    if args.dev:\n",
    "        print(\"-------------\\nDev\")\n",
    "        args.dev_stats = train.compute_threshold_and_dev_stats(dev_data, model, args)\n",
    "        print(\"Save dev results to {}\".format(save_path))\n",
    "        args_dict = vars(args)\n",
    "        pickle.dump(args_dict, open(save_path, 'wb'))\n",
    "\n",
    "    if args.test:\n",
    "\n",
    "        print(\"-------------\\nTest\")\n",
    "        args.test_stats = train.eval_model(test_data, model, args)\n",
    "        print(\"Save test results to {}\".format(save_path))\n",
    "        args_dict = vars(args)\n",
    "        pickle.dump(args_dict, open(save_path, 'wb'))\n",
    "\n",
    "    if (args.dev or args.test) and args.prediction_save_path is not None:\n",
    "        exams, probs = [], []\n",
    "        if args.dev:\n",
    "            exams.extend( args.dev_stats['exams'])\n",
    "            probs.extend( args.dev_stats['probs'])\n",
    "        if args.test:\n",
    "            exams.extend( args.test_stats['exams'])\n",
    "            probs.extend( args.test_stats['probs'])\n",
    "        legend = ['patient_exam_id']\n",
    "        if args.callibrator_snapshot is not None:\n",
    "            #callibrator = pickle.load(open(args.callibrator_snapshot,'rb'))\n",
    "            callibrator = pickle.load(open(args.callibrator_snapshot,'rb'), fix_imports=True, encoding=\"latin1\") #added\n",
    "        for i in range(args.max_followup):\n",
    "            legend.append(\"{}_year_risk\".format(i+1))\n",
    "        export = {}\n",
    "        with open(args.prediction_save_path,'w') as out_file:\n",
    "            writer = csv.DictWriter(out_file, fieldnames=legend)\n",
    "            writer.writeheader()\n",
    "            for exam, arr in zip(exams, probs):\n",
    "                export['patient_exam_id'] = exam\n",
    "                for i in range(args.max_followup):\n",
    "                    key = \"{}_year_risk\".format(i+1)\n",
    "                    raw_val = arr[i]\n",
    "                    if args.callibrator_snapshot is not None:\n",
    "                        val = callibrator[i].predict_proba([[raw_val]])[0,1]\n",
    "                        #val = CalibratedClassifierCV[i].predict_proba([[raw_val]])[0,1] #error\n",
    "                    else:\n",
    "                        val = raw_val\n",
    "                    export[key] = val\n",
    "                writer.writerow(export)\n",
    "        print(\"Exported predictions to {}\".format(args.prediction_save_path))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
